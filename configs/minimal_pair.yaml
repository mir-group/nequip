# Minimal example of training a pair potential.

# general
root: results/aspirin
run_name: minimal-pair
seed: 123
dataset_seed: 456

# network
# For only a pair potential:
# model_builders:
#   - PairPotential
#   - StressForceOutput
#   - RescaleEnergyEtc
# For a pair potential term with a neural network model on top:
model_builders:
  - SimpleIrrepsConfig
  - EnergyModel
  - PerSpeciesRescale
  - PairPotentialTerm   # MUST come after PerSpeciesRescale
  - StressForceOutput
  - RescaleEnergyEtc

# neural network
num_basis: 8
r_max: 4.0
l_max: 2
parity: true
num_features: 16

# pair potential
# pair_style: LJ
# lj_sigma: 0.5
# lj_delta: 0.0
# lj_epsilon: 0.05
# lj_sigma_trainable: true
# lj_delta_trainable: false
# lj_epsilon_trainable: true
pair_style: ZBL
units: real  # Ang and kcal/mol

# data set
# the keys used need to be stated at least once in key_mapping, npz_fixed_field_keys or npz_keys
# key_mapping is used to map the key in the npz file to the NequIP default values (see data/_key.py)
# all arrays are expected to have the shape of (nframe, natom, ?) except the fixed fields
# note that if your data set uses pbc, you need to also pass an array that maps to the nequip "pbc" key
dataset: npz                                                                       # type of data set, can be npz or ase
dataset_url: http://quantum-machine.org/gdml/data/npz/aspirin_ccsd.zip             # url to download the npz. optional
dataset_file_name: ./benchmark_data/aspirin_ccsd-train.npz                         # path to data set file
key_mapping:
  z: atomic_numbers                                                                # atomic species, integers
  E: total_energy                                                                  # total potential eneriges to train to
  F: forces                                                                        # atomic forces to train to
  R: pos                                                                           # raw atomic positions
npz_fixed_field_keys:                                                              # fields that are repeated across different examples
  - atomic_numbers

chemical_symbols:
  - H
  - O
  - C

# logging
wandb: false
# verbose: debug

# training
n_train: 150                                                                       # number of training data
n_val: 50                                                                          # number of validation data
learning_rate: 0.005                                                               # learning rate, we found values between 0.01 and 0.005 to work best - this is often one of the most important hyperparameters to tune
batch_size: 5                                                                      # batch size, we found it important to keep this small for most applications including forces (1-5); for energy-only training, higher batch sizes work better
validation_batch_size: 10                                                          # batch size for evaluating the model during validation. This does not affect the training results, but using the highest value possible (<=n_val) without running out of memory will speed up your training.
max_epochs: 100000                   
append: true

# loss function
loss_coeffs:                                                                        
  forces: 1                                                                        # if using PerAtomMSELoss, a default weight of 1:1 on each should work well
  total_energy:                                                                    
    - 1
    - PerAtomMSELoss

# optimizer
optimizer_name: Adam
